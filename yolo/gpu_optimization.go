package yolo

import (
	"context"
	"fmt"
	"runtime"
	"sync"
	"time"
)

// HighEndGPUOptimizedConfig é«˜ç«¯GPUæè‡´ä¼˜åŒ–é…ç½®
// æ”¯æŒé«˜ç«¯æ˜¾å¡ï¼Œå¼ºåˆ¶è¦æ±‚GPUå¯ç”¨
// æ³¨æ„ï¼šæ­¤é…ç½®å¼ºåˆ¶è¦æ±‚GPUï¼Œå¦‚æœGPUä¸å¯ç”¨ä¼šåœ¨NewYOLOæ—¶è¿”å›é”™è¯¯
func HighEndGPUOptimizedConfig() *YOLOConfig {
	return HighPerformanceGPUConfig() // å‘åå…¼å®¹
}

// HighPerformanceGPUConfig é«˜æ€§èƒ½GPUä¸“ç”¨æè‡´ä¼˜åŒ–é…ç½®
// é’ˆå¯¹é«˜ç«¯æ˜¾å¡çš„å¤§æ˜¾å­˜å’Œå¤šæ ¸å¿ƒè¿›è¡Œä¼˜åŒ–
// æ³¨æ„ï¼šæ­¤é…ç½®å¼ºåˆ¶è¦æ±‚GPUï¼Œå¦‚æœGPUä¸å¯ç”¨ä¼šåœ¨NewYOLOæ—¶è¿”å›é”™è¯¯
func HighPerformanceGPUConfig() *YOLOConfig {
	config := &YOLOConfig{
		InputSize:      640,
		UseGPU:         true,  // å¼ºåˆ¶è¦æ±‚GPU
		GPUDeviceID:    0,
		UseCUDA:        true,  // å¼ºåˆ¶è¦æ±‚CUDA
		CUDADeviceID:   0,
		CUDAMemoryPool: true,
		LibraryPath:    "",
	}

	fmt.Println("ğŸš€ é«˜æ€§èƒ½GPUæè‡´ä¼˜åŒ–é…ç½®ï¼šå¤§æ˜¾å­˜+å¤šCUDAæ ¸å¿ƒ")
	fmt.Println("âš ï¸  æ³¨æ„ï¼šæ­¤é…ç½®å¼ºåˆ¶è¦æ±‚GPUï¼Œå¦‚æœGPUä¸å¯ç”¨å°†è¿”å›é”™è¯¯")
	fmt.Println("ğŸ’¡ å¦‚éœ€è‡ªåŠ¨é€‚é…ï¼Œè¯·ä½¿ç”¨ DefaultConfig().WithGPU(true)")

	return config
}

// NewHighEndGPUVideoOptimization åˆ›å»ºé«˜ç«¯GPUé€šç”¨è§†é¢‘ä¼˜åŒ–å®ä¾‹
// è‡ªåŠ¨æ£€æµ‹æ˜¾å­˜å¤§å°å¹¶è°ƒæ•´é…ç½®ï¼šå¤§æ˜¾å­˜æ˜¾å¡(20GB+), ä¸­é«˜ç«¯æ˜¾å¡(12-16GB), ä¸­ç«¯æ˜¾å¡(8-10GB)ç­‰
func NewHighEndGPUVideoOptimization() *VideoOptimization {
	return NewHighPerformanceGPUVideoOptimization() // ä½¿ç”¨æœ€é«˜é…ç½®ä½œä¸ºé»˜è®¤
}

// NewAdaptiveGPUVideoOptimization åˆ›å»ºè‡ªé€‚åº”GPUè§†é¢‘ä¼˜åŒ–å®ä¾‹
// æ ¹æ®æ£€æµ‹åˆ°çš„æ˜¾å­˜å¤§å°è‡ªåŠ¨è°ƒæ•´æ‰¹å¤„ç†å’Œå†…å­˜æ± é…ç½®
func NewAdaptiveGPUVideoOptimization() *VideoOptimization {
	cpuCores := runtime.NumCPU()

	// æ£€æµ‹æ˜¾å­˜å¤§å°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥é€šè¿‡CUDA APIè·å–ï¼‰
	vramGB := detectVRAMSize() // å‡è®¾è¿™ä¸ªå‡½æ•°å­˜åœ¨

	var batchSize, maxBatchSize, parallelWorkers int
	var memoryPoolGB int64
	var gcInterval int64

	// æ ¹æ®æ˜¾å­˜å¤§å°è°ƒæ•´é…ç½®
	switch {
	case vramGB >= 20: // å¤§æ˜¾å­˜æ˜¾å¡ (20GB+)
			batchSize = cpuCores * 8
			maxBatchSize = cpuCores * 16
			parallelWorkers = cpuCores * 6
			memoryPoolGB = 20
		case vramGB >= 12: // ä¸­é«˜ç«¯æ˜¾å¡ (12-16GB)
			batchSize = cpuCores * 6
			maxBatchSize = cpuCores * 12
			parallelWorkers = cpuCores * 4
			memoryPoolGB = 12
		case vramGB >= 8: // ä¸­ç«¯æ˜¾å¡ (8-10GB)
			batchSize = cpuCores * 4
			maxBatchSize = cpuCores * 8
			parallelWorkers = cpuCores * 3
			memoryPoolGB = 8
		parallelWorkers = cpuCores * 3
		memoryPoolGB = 6
		gcInterval = 20
	default: // å…¶ä»–GPU
		batchSize = cpuCores * 2
		maxBatchSize = cpuCores * 4
		parallelWorkers = cpuCores * 2
		memoryPoolGB = 4
		gcInterval = 15
	}

	fmt.Printf("ğŸš€ æ£€æµ‹åˆ°æ˜¾å­˜: %dGBï¼Œä½¿ç”¨ä¼˜åŒ–é…ç½®: æ‰¹å¤„ç†=%d, æœ€å¤§æ‰¹å¤„ç†=%d, å†…å­˜æ± =%dGB\n",
		vramGB, batchSize, maxBatchSize, memoryPoolGB)

	// é¢„åˆ†é…å†…å­˜ç¼“å†²åŒº
	preprocessBuf := make([][]float32, batchSize)
	memoryBuffer := make([][]float32, maxBatchSize)
	for i := range memoryBuffer {
		memoryBuffer[i] = make([]float32, 3*640*640)
	}

	// åˆ›å»ºå¯¹è±¡æ± 
	imagePool := &sync.Pool{
		New: func() interface{} {
			return make([]float32, 3*640*640)
		},
	}

	preprocessPool := &sync.Pool{
		New: func() interface{} {
			return make([]float32, 3*640*640)
		},
	}

	resultPool := &sync.Pool{
		New: func() interface{} {
			return make([]Detection, 0, 150)
		},
	}

	// åˆ›å»ºå¼‚æ­¥å¤„ç†é˜Ÿåˆ—
	asyncQueue := make(chan *ProcessTask, maxBatchSize*3)
	processDone := make(chan *ProcessResult, maxBatchSize*3)
	workerPool := make(chan struct{}, parallelWorkers)

	// å¡«å……å·¥ä½œæ± 
	for i := 0; i < parallelWorkers; i++ {
		workerPool <- struct{}{}
	}

	// åˆ›å»ºä¸Šä¸‹æ–‡
	ctx, cancel := context.WithCancel(context.Background())

	// åˆ›å»ºè‡ªé€‚åº”CUDAåŠ é€Ÿå™¨
	cudaAccelerator, err := NewAdaptiveCUDAAccelerator(0, memoryPoolGB)
	if err != nil {
		fmt.Printf("âš ï¸ è‡ªé€‚åº”CUDAåŠ é€Ÿå™¨åˆ›å»ºå¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†æ¨¡å¼: %v\n", err)
		cudaAccelerator = nil
	}

	vo := &VideoOptimization{
		batchSize:       batchSize,
		preprocessBuf:   preprocessBuf,
		imagePool:       imagePool,
		enableGPU:       true,
		maxBatchSize:    maxBatchSize,
		preprocessPool:  preprocessPool,
		resultPool:      resultPool,
		parallelWorkers: parallelWorkers,
		memoryBuffer:    memoryBuffer,
		asyncQueue:      asyncQueue,
		processDone:     processDone,
		workerPool:      workerPool,
		cudaAccelerator: cudaAccelerator,
		enableCUDA:      cudaAccelerator != nil,
		cudaDeviceID:    0,
		circuitBreaker:  &CircuitBreaker{maxFailures: 10, timeout: 30 * time.Second, retryTimeout: 5 * time.Second},
		rateLimiter:     &RateLimiter{maxTokens: int64(maxBatchSize * 2), refillRate: int64(maxBatchSize)},
		resourceMonitor: &ResourceMonitor{maxMemory: memoryPoolGB * 1024 * 1024 * 1024, maxGoroutines: 1000, maxCPU: 90.0, checkInterval: time.Second},
		healthChecker:   &HealthChecker{checkInterval: 5 * time.Second, maxFailures: 5},
		metrics:         &PerformanceMetrics{minLatency: time.Hour},
		ctx:             ctx,
		cancel:          cancel,
		gcInterval:      gcInterval,
		lastGCTime:      time.Now(),
	}

	// å¯åŠ¨å¼‚æ­¥å·¥ä½œçº¿ç¨‹å’Œç›‘æ§
	vo.startAsyncWorkers()
	vo.startStabilityMonitors()

	return vo
}

// NewHighPerformanceGPUVideoOptimization åˆ›å»ºé«˜æ€§èƒ½GPUä¸“ç”¨è§†é¢‘ä¼˜åŒ–å®ä¾‹
func NewHighPerformanceGPUVideoOptimization() *VideoOptimization {
	cpuCores := runtime.NumCPU()

	// é«˜æ€§èƒ½GPUä¸“ç”¨é…ç½® - å……åˆ†åˆ©ç”¨å¤§æ˜¾å­˜
	batchSize := cpuCores * 8       // å¤§æ‰¹å¤„ç†ï¼Œåˆ©ç”¨å¤§æ˜¾å­˜
	maxBatchSize := cpuCores * 16   // æå¤§æ‰¹å¤„ç†
	parallelWorkers := cpuCores * 6 // æ›´å¤šå¹¶è¡Œå·¥ä½œçº¿ç¨‹

	// é¢„åˆ†é…æ›´å¤§çš„å†…å­˜ç¼“å†²åŒºï¼Œå……åˆ†åˆ©ç”¨24GBæ˜¾å­˜
	preprocessBuf := make([][]float32, batchSize)
	memoryBuffer := make([][]float32, maxBatchSize)
	for i := range memoryBuffer {
		memoryBuffer[i] = make([]float32, 3*640*640) // 640x640ç¼“å†²åŒº
	}

	// åˆ›å»ºå¯¹è±¡æ± ï¼Œä½¿ç”¨æ›´å¤§çš„ç¼“å†²åŒº
	imagePool := &sync.Pool{
		New: func() interface{} {
			return make([]float32, 3*640*640)
		},
	}

	preprocessPool := &sync.Pool{
		New: func() interface{} {
			return make([]float32, 3*640*640)
		},
	}

	resultPool := &sync.Pool{
		New: func() interface{} {
			return make([]Detection, 0, 200) // æ›´å¤§çš„æ£€æµ‹ç»“æœé¢„åˆ†é…
		},
	}

	// åˆ›å»ºæ›´å¤§çš„å¼‚æ­¥å¤„ç†é˜Ÿåˆ—
	asyncQueue := make(chan *ProcessTask, maxBatchSize*4)
	processDone := make(chan *ProcessResult, maxBatchSize*4)
	workerPool := make(chan struct{}, parallelWorkers)

	// å¡«å……å·¥ä½œæ± 
	for i := 0; i < parallelWorkers; i++ {
		workerPool <- struct{}{}
	}

	// åˆ›å»ºä¸Šä¸‹æ–‡ç”¨äºä¼˜é›…å…³é—­
	ctx, cancel := context.WithCancel(context.Background())

	// åˆ›å»ºé«˜æ€§èƒ½GPUä¸“ç”¨CUDAåŠ é€Ÿå™¨
	cudaAccelerator, err := NewHighPerformanceGPUCUDAAccelerator(0)
	if err != nil {
		fmt.Printf("âš ï¸ é«˜æ€§èƒ½GPU CUDAåŠ é€Ÿå™¨åˆ›å»ºå¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†æ¨¡å¼: %v\n", err)
		cudaAccelerator = nil
	} else {
		fmt.Printf("ğŸš€ é«˜æ€§èƒ½GPU CUDAåŠ é€Ÿå™¨åˆå§‹åŒ–æˆåŠŸï¼Œè®¾å¤‡ID: %d\n", 0)
	}

	vo := &VideoOptimization{
		batchSize:       batchSize,
		preprocessBuf:   preprocessBuf,
		imagePool:       imagePool,
		enableGPU:       true,
		maxBatchSize:    maxBatchSize,
		preprocessPool:  preprocessPool,
		resultPool:      resultPool,
		parallelWorkers: parallelWorkers,
		memoryBuffer:    memoryBuffer,
		asyncQueue:      asyncQueue,
		processDone:     processDone,
		workerPool:      workerPool,
		cudaAccelerator: cudaAccelerator,
		enableCUDA:      cudaAccelerator != nil,
		cudaDeviceID:    0,
		circuitBreaker:  &CircuitBreaker{maxFailures: 10, timeout: 30 * time.Second, retryTimeout: 5 * time.Second},
		rateLimiter:     &RateLimiter{maxTokens: int64(maxBatchSize * 2), refillRate: int64(maxBatchSize)},
		resourceMonitor: &ResourceMonitor{maxMemory: 20 * 1024 * 1024 * 1024, maxGoroutines: 1000, maxCPU: 90.0, checkInterval: time.Second},
		healthChecker:   &HealthChecker{checkInterval: 5 * time.Second, maxFailures: 5},
		metrics:         &PerformanceMetrics{minLatency: time.Hour},
		ctx:             ctx,
		cancel:          cancel,
		gcInterval:      30, // é«˜æ€§èƒ½GPUæ˜¾å­˜å¤§ï¼Œå¯ä»¥å‡å°‘GCé¢‘ç‡
		lastGCTime:      time.Now(),
	}

	// å¯åŠ¨å¼‚æ­¥å·¥ä½œçº¿ç¨‹å’Œç›‘æ§
	vo.startAsyncWorkers()
	vo.startStabilityMonitors()

	return vo
}

// detectVRAMSize æ£€æµ‹æ˜¾å­˜å¤§å°ï¼ˆGBï¼‰
// ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥é€šè¿‡CUDA APIè·å–å‡†ç¡®ä¿¡æ¯
func detectVRAMSize() int {
	// è¿™é‡Œåº”è¯¥è°ƒç”¨CUDA APIè·å–å®é™…æ˜¾å­˜å¤§å°
	// ç›®å‰è¿”å›ä¸€ä¸ªä¼°ç®—å€¼ï¼Œå¯ä»¥æ ¹æ®GPUå‹å·åˆ¤æ–­
	// å®é™…å®ç°ä¸­åº”è¯¥ä½¿ç”¨ cudaMemGetInfo ç­‰API
	return 24 // é»˜è®¤å‡è®¾ä¸ºé«˜ç«¯GPU
}

// NewAdaptiveCUDAAccelerator åˆ›å»ºè‡ªé€‚åº”CUDAåŠ é€Ÿå™¨
// æ ¹æ®æ˜¾å­˜å¤§å°è‡ªåŠ¨è°ƒæ•´å†…å­˜æ± å’Œæ‰¹å¤„ç†é…ç½®
func NewAdaptiveCUDAAccelerator(deviceID int, memoryPoolGB int64) (*CUDAAccelerator, error) {
	// æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
	if !isCUDAAvailable() {
		return nil, fmt.Errorf("CUDAä¸å¯ç”¨")
	}

	cpuCores := runtime.NumCPU()

	// æ ¹æ®æ˜¾å­˜å¤§å°è°ƒæ•´æµæ•°é‡å’Œæ‰¹å¤„ç†å¤§å°
	var streamCount, batchSize int
	switch {
	case memoryPoolGB >= 20: // é«˜ç«¯GPU (20GB+æ˜¾å­˜)
		streamCount = cpuCores * 4
		batchSize = cpuCores * 64
	case memoryPoolGB >= 12: // ä¸­é«˜ç«¯GPU (12-16GBæ˜¾å­˜)
		streamCount = cpuCores * 3
		batchSize = cpuCores * 48
	case memoryPoolGB >= 8: // ä¸­ç«¯GPU (8-10GBæ˜¾å­˜)
		streamCount = cpuCores * 2
		batchSize = cpuCores * 32
	default: // å…¶ä»–GPU
		streamCount = cpuCores * 2
		batchSize = cpuCores * 16
	}

	// åˆ›å»ºå†…å­˜æ± 
	memoryPool, err := newCUDAMemoryPool(deviceID, memoryPoolGB*1024*1024*1024)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºè‡ªé€‚åº”CUDAå†…å­˜æ± å¤±è´¥: %v", err)
	}

	// åˆ›å»ºæµç®¡ç†å™¨
	streamManager, err := newCUDAStreamManager(streamCount)
	if err != nil {
		memoryPool.Destroy()
		return nil, fmt.Errorf("åˆ›å»ºè‡ªé€‚åº”CUDAæµç®¡ç†å™¨å¤±è´¥: %v", err)
	}

	// åˆ›å»ºé¢„å¤„ç†å™¨
	preprocessor, err := newCUDAPreprocessor(deviceID)
	if err != nil {
		streamManager.Destroy()
		memoryPool.Destroy()
		return nil, fmt.Errorf("åˆ›å»ºè‡ªé€‚åº”CUDAé¢„å¤„ç†å™¨å¤±è´¥: %v", err)
	}

	// åˆ›å»ºæ‰¹å¤„ç†å™¨
	batchProcessor, err := newCUDABatchProcessor(batchSize)
	if err != nil {
		preprocessor.Destroy()
		streamManager.Destroy()
		memoryPool.Destroy()
		return nil, fmt.Errorf("åˆ›å»ºè‡ªé€‚åº”CUDAæ‰¹å¤„ç†å™¨å¤±è´¥: %v", err)
	}

	// åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨
	performanceMonitor := newCUDAPerformanceMonitor()

	return &CUDAAccelerator{
		enabled:            true,
		deviceID:           deviceID,
		streamCount:        streamCount,
		memoryPool:         memoryPool,
		streamManager:      streamManager,
		preprocessor:       preprocessor,
		batchProcessor:     batchProcessor,
		performanceMonitor: performanceMonitor,
	}, nil
}

// NewHighPerformanceGPUCUDAAccelerator åˆ›å»ºé«˜æ€§èƒ½GPUä¸“ç”¨CUDAåŠ é€Ÿå™¨
func NewHighPerformanceGPUCUDAAccelerator(deviceID int) (*CUDAAccelerator, error) {
	// æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
	if !isCUDAAvailable() {
		return nil, fmt.Errorf("CUDAä¸å¯ç”¨")
	}

	cpuCores := runtime.NumCPU()
	streamCount := cpuCores * 4 // é«˜æ€§èƒ½GPUå¯ä»¥æ”¯æŒæ›´å¤šæµ

	// åˆ›å»ºæ›´å¤§çš„å†…å­˜æ±  - å……åˆ†åˆ©ç”¨é«˜æ€§èƒ½GPUçš„å¤§æ˜¾å­˜
	memoryPool, err := newCUDAMemoryPool(deviceID, 20*1024*1024*1024) // 20GBå†…å­˜æ± 
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºé«˜æ€§èƒ½GPU CUDAå†…å­˜æ± å¤±è´¥: %v", err)
	}

	// åˆ›å»ºæµç®¡ç†å™¨
	streamManager, err := newCUDAStreamManager(streamCount)
	if err != nil {
		memoryPool.Destroy()
		return nil, fmt.Errorf("åˆ›å»ºé«˜æ€§èƒ½GPU CUDAæµç®¡ç†å™¨å¤±è´¥: %v", err)
	}

	// åˆ›å»ºé¢„å¤„ç†å™¨
	preprocessor, err := newCUDAPreprocessor(deviceID)
	if err != nil {
		streamManager.Destroy()
		memoryPool.Destroy()
		return nil, fmt.Errorf("åˆ›å»ºé«˜æ€§èƒ½GPU CUDAé¢„å¤„ç†å™¨å¤±è´¥: %v", err)
	}

	// åˆ›å»ºæ‰¹å¤„ç†å™¨ - é«˜æ€§èƒ½GPUå¯ä»¥å¤„ç†æ›´å¤§çš„æ‰¹æ¬¡
	batchProcessor, err := newCUDABatchProcessor(cpuCores * 64) // è¶…å¤§æ‰¹å¤„ç†
	if err != nil {
		preprocessor.Destroy()
		streamManager.Destroy()
		memoryPool.Destroy()
		return nil, fmt.Errorf("åˆ›å»ºé«˜æ€§èƒ½GPU CUDAæ‰¹å¤„ç†å™¨å¤±è´¥: %v", err)
	}

	// åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨
	performanceMonitor := newCUDAPerformanceMonitor()

	return &CUDAAccelerator{
		enabled:            true,
		deviceID:           deviceID,
		streamCount:        streamCount,
		memoryPool:         memoryPool,
		streamManager:      streamManager,
		preprocessor:       preprocessor,
		batchProcessor:     batchProcessor,
		performanceMonitor: performanceMonitor,
	}, nil
}

// HighEndGPUPerformanceTips é«˜ç«¯GPUæ€§èƒ½ä¼˜åŒ–å»ºè®®
func HighEndGPUPerformanceTips() {
	fmt.Println("\nğŸš€ é«˜ç«¯GPUæ€§èƒ½ä¼˜åŒ–å»ºè®®:")
	fmt.Println("1. ä½¿ç”¨ HighEndGPUOptimizedConfig() æˆ– NewAdaptiveGPUVideoOptimization() è‡ªåŠ¨é…ç½®")
	fmt.Println("2. æ ¹æ®æ˜¾å­˜å¤§å°é€‰æ‹©åˆé€‚çš„é…ç½®:")
	fmt.Println("   - é«˜ç«¯GPU (20GB+æ˜¾å­˜): æ‰¹å¤„ç†128+, å†…å­˜æ± 20GB")
	fmt.Println("   - ä¸­é«˜ç«¯GPU (12-16GBæ˜¾å­˜): æ‰¹å¤„ç†96+, å†…å­˜æ± 12GB")
	fmt.Println("   - ä¸­ç«¯GPU (8-10GBæ˜¾å­˜): æ‰¹å¤„ç†64+, å†…å­˜æ± 6GB")
	fmt.Println("3. å¹¶è¡Œå·¥ä½œçº¿ç¨‹: æ ¹æ®æ˜¾å­˜è‡ªåŠ¨è°ƒæ•´ (CPUæ ¸å¿ƒæ•° * 2-6)")
	fmt.Println("4. CUDAæµæ•°é‡: æ ¹æ®æ˜¾å­˜è‡ªåŠ¨è°ƒæ•´ (CPUæ ¸å¿ƒæ•° * 2-4)")
	fmt.Println("5. GCé—´éš”: æ˜¾å­˜è¶Šå¤§é—´éš”è¶Šé•¿ (15-30å¸§)")
	fmt.Println("6. ç¡®ä¿CUDA 11.8+ å’Œæœ€æ–°é©±åŠ¨")
	fmt.Println("7. å…³é—­ä¸å¿…è¦çš„åå°ç¨‹åºé‡Šæ”¾æ˜¾å­˜")
	fmt.Println("8. ä½¿ç”¨ TensorRT è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹")
	fmt.Println("9. ç›‘æ§GPUåˆ©ç”¨ç‡ï¼Œç¡®ä¿è¾¾åˆ°90%+")
	fmt.Println("10. è€ƒè™‘ä½¿ç”¨æ··åˆç²¾åº¦(FP16)æå‡æ€§èƒ½\n")
}

// HighPerformanceGPUTips é«˜æ€§èƒ½GPUæ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼ˆå‘åå…¼å®¹ï¼‰
func HighPerformanceGPUTips() {
	HighEndGPUPerformanceTips()
}

// GetGPUBenchmarkConfig è·å–GPUåŸºå‡†æµ‹è¯•é…ç½®
// æ ¹æ®æ˜¾å­˜å¤§å°è¿”å›ç›¸åº”çš„æ€§èƒ½é¢„æœŸ
func GetGPUBenchmarkConfig(vramGB int) map[string]interface{} {
	cpuCores := runtime.NumCPU()

	switch {
	case vramGB >= 20: // é«˜ç«¯GPU, å¤§æ˜¾å­˜
		return map[string]interface{}{
			"gpu_tier":           "æ——èˆ°çº§ (20GB+æ˜¾å­˜)",
			"vram_size":          fmt.Sprintf("%dGB", vramGB),
			"memory_pool_size":   "20GB",
			"batch_size":         cpuCores * 8,
			"max_batch_size":     cpuCores * 16,
			"parallel_workers":   cpuCores * 6,
			"cuda_streams":       cpuCores * 4,
			"gc_interval":        30,
			"expected_fps":       "300-500 (1000å¸§è§†é¢‘)",
			"target_time":        "10-20ç§’ (1000å¸§è§†é¢‘)",
			"optimization_level": "æè‡´",
		}
	case vramGB >= 12: // ä¸­é«˜ç«¯GPU
		return map[string]interface{}{
			"gpu_tier":           "é«˜ç«¯çº§ (12-16GBæ˜¾å­˜)",
			"vram_size":          fmt.Sprintf("%dGB", vramGB),
			"memory_pool_size":   "12GB",
			"batch_size":         cpuCores * 6,
			"max_batch_size":     cpuCores * 12,
			"parallel_workers":   cpuCores * 4,
			"cuda_streams":       cpuCores * 3,
			"gc_interval":        25,
			"expected_fps":       "200-350 (1000å¸§è§†é¢‘)",
			"target_time":        "15-30ç§’ (1000å¸§è§†é¢‘)",
			"optimization_level": "é«˜çº§",
		}
	case vramGB >= 8: // ä¸­ç«¯GPU
		return map[string]interface{}{
			"gpu_tier":           "ä¸­é«˜ç«¯çº§ (8-10GBæ˜¾å­˜)",
			"vram_size":          fmt.Sprintf("%dGB", vramGB),
			"memory_pool_size":   "6GB",
			"batch_size":         cpuCores * 4,
			"max_batch_size":     cpuCores * 8,
			"parallel_workers":   cpuCores * 3,
			"cuda_streams":       cpuCores * 2,
			"gc_interval":        20,
			"expected_fps":       "150-250 (1000å¸§è§†é¢‘)",
			"target_time":        "20-40ç§’ (1000å¸§è§†é¢‘)",
			"optimization_level": "ä¸­çº§",
		}
	default: // å…¶ä»–GPU
		return map[string]interface{}{
			"gpu_tier":           "æ ‡å‡†çº§",
			"vram_size":          fmt.Sprintf("%dGB", vramGB),
			"memory_pool_size":   "4GB",
			"batch_size":         cpuCores * 2,
			"max_batch_size":     cpuCores * 4,
			"parallel_workers":   cpuCores * 2,
			"cuda_streams":       cpuCores * 2,
			"gc_interval":        15,
			"expected_fps":       "100-180 (1000å¸§è§†é¢‘)",
			"target_time":        "30-60ç§’ (1000å¸§è§†é¢‘)",
			"optimization_level": "åŸºç¡€",
		}
	}
}

// HighPerformanceGPUBenchmarkConfig é«˜æ€§èƒ½GPUåŸºå‡†æµ‹è¯•é…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
func HighPerformanceGPUBenchmarkConfig() map[string]interface{} {
	return GetGPUBenchmarkConfig(24) // é«˜æ€§èƒ½GPU æœ‰å¤§æ˜¾å­˜
}

// GetOptimalGPUSettings è·å–å½“å‰GPUçš„æœ€ä¼˜è®¾ç½®å»ºè®®
func GetOptimalGPUSettings() map[string]interface{} {
	vramGB := detectVRAMSize()
	config := GetGPUBenchmarkConfig(vramGB)

	fmt.Printf("ğŸ” æ£€æµ‹åˆ°GPUé…ç½®: %s\n", config["gpu_tier"])
	fmt.Printf("ğŸ“Š é¢„æœŸæ€§èƒ½: %s\n", config["expected_fps"])
	fmt.Printf("â±ï¸  ç›®æ ‡å¤„ç†æ—¶é—´: %s\n", config["target_time"])

	return config
}
