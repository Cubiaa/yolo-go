package yolo

import (
	"context"
	"fmt"
	"runtime"
	"sync"
	"time"
)

// HighEndGPUOptimizedConfig é«˜ç«¯GPUæè‡´ä¼˜åŒ–é…ç½®
// æ”¯æŒRTX 4090/4080/3090ç­‰é«˜ç«¯æ˜¾å¡ï¼Œè‡ªåŠ¨æ£€æµ‹æ˜¾å­˜å¤§å°è¿›è¡Œä¼˜åŒ–
func HighEndGPUOptimizedConfig() *YOLOConfig {
	return RTX4090OptimizedConfig() // å‘åå…¼å®¹
}

// RTX4090OptimizedConfig RTX 4090ä¸“ç”¨æè‡´ä¼˜åŒ–é…ç½®
// é’ˆå¯¹RTX 4090çš„24GBæ˜¾å­˜å’Œ10752ä¸ªCUDAæ ¸å¿ƒè¿›è¡Œä¼˜åŒ–
func RTX4090OptimizedConfig() *YOLOConfig {
	config := &YOLOConfig{
		InputSize:      640, // ä¿æŒ640ä»¥å¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦
		UseGPU:         true,
		GPUDeviceID:    0,
		UseCUDA:        true,
		CUDADeviceID:   0,
		CUDAMemoryPool: true,
		LibraryPath:    "",
	}

	// æ£€æŸ¥GPUå’ŒCUDAå¯ç”¨æ€§
	if !IsGPUAvailable() {
		config.UseGPU = false
		config.UseCUDA = false
		fmt.Println("âš ï¸ GPUä¸å¯ç”¨ï¼ŒRTX 4090é…ç½®å·²å›é€€åˆ°CPUæ¨¡å¼")
	} else {
		fmt.Println("ğŸš€ RTX 4090æè‡´ä¼˜åŒ–é…ç½®ï¼š24GBæ˜¾å­˜+10752 CUDAæ ¸å¿ƒ")
	}

	return config
}

// NewHighEndGPUVideoOptimization åˆ›å»ºé«˜ç«¯GPUé€šç”¨è§†é¢‘ä¼˜åŒ–å®ä¾‹
// è‡ªåŠ¨æ£€æµ‹æ˜¾å­˜å¤§å°å¹¶è°ƒæ•´é…ç½®ï¼šRTX 4090(24GB), RTX 4080(16GB), RTX 3090(24GB)ç­‰
func NewHighEndGPUVideoOptimization() *VideoOptimization {
	return NewRTX4090VideoOptimization() // ä½¿ç”¨æœ€é«˜é…ç½®ä½œä¸ºé»˜è®¤
}

// NewAdaptiveGPUVideoOptimization åˆ›å»ºè‡ªé€‚åº”GPUè§†é¢‘ä¼˜åŒ–å®ä¾‹
// æ ¹æ®æ£€æµ‹åˆ°çš„æ˜¾å­˜å¤§å°è‡ªåŠ¨è°ƒæ•´æ‰¹å¤„ç†å’Œå†…å­˜æ± é…ç½®
func NewAdaptiveGPUVideoOptimization() *VideoOptimization {
	cpuCores := runtime.NumCPU()

	// æ£€æµ‹æ˜¾å­˜å¤§å°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥é€šè¿‡CUDA APIè·å–ï¼‰
	vramGB := detectVRAMSize() // å‡è®¾è¿™ä¸ªå‡½æ•°å­˜åœ¨

	var batchSize, maxBatchSize, parallelWorkers int
	var memoryPoolGB int64
	var gcInterval int64

	// æ ¹æ®æ˜¾å­˜å¤§å°è°ƒæ•´é…ç½®
	switch {
	case vramGB >= 20: // RTX 4090 (24GB), RTX 3090 (24GB)
		batchSize = cpuCores * 8
		maxBatchSize = cpuCores * 16
		parallelWorkers = cpuCores * 6
		memoryPoolGB = 20
		gcInterval = 30
	case vramGB >= 12: // RTX 4080 (16GB), RTX 3080 Ti (12GB)
		batchSize = cpuCores * 6
		maxBatchSize = cpuCores * 12
		parallelWorkers = cpuCores * 4
		memoryPoolGB = 12
		gcInterval = 25
	case vramGB >= 8: // RTX 3070 Ti (8GB), RTX 3080 (10GB)
		batchSize = cpuCores * 4
		maxBatchSize = cpuCores * 8
		parallelWorkers = cpuCores * 3
		memoryPoolGB = 6
		gcInterval = 20
	default: // å…¶ä»–GPU
		batchSize = cpuCores * 2
		maxBatchSize = cpuCores * 4
		parallelWorkers = cpuCores * 2
		memoryPoolGB = 4
		gcInterval = 15
	}

	fmt.Printf("ğŸš€ æ£€æµ‹åˆ°æ˜¾å­˜: %dGBï¼Œä½¿ç”¨ä¼˜åŒ–é…ç½®: æ‰¹å¤„ç†=%d, æœ€å¤§æ‰¹å¤„ç†=%d, å†…å­˜æ± =%dGB\n",
		vramGB, batchSize, maxBatchSize, memoryPoolGB)

	// é¢„åˆ†é…å†…å­˜ç¼“å†²åŒº
	preprocessBuf := make([][]float32, batchSize)
	memoryBuffer := make([][]float32, maxBatchSize)
	for i := range memoryBuffer {
		memoryBuffer[i] = make([]float32, 3*640*640)
	}

	// åˆ›å»ºå¯¹è±¡æ± 
	imagePool := &sync.Pool{
		New: func() interface{} {
			return make([]float32, 3*640*640)
		},
	}

	preprocessPool := &sync.Pool{
		New: func() interface{} {
			return make([]float32, 3*640*640)
		},
	}

	resultPool := &sync.Pool{
		New: func() interface{} {
			return make([]Detection, 0, 150)
		},
	}

	// åˆ›å»ºå¼‚æ­¥å¤„ç†é˜Ÿåˆ—
	asyncQueue := make(chan *ProcessTask, maxBatchSize*3)
	processDone := make(chan *ProcessResult, maxBatchSize*3)
	workerPool := make(chan struct{}, parallelWorkers)

	// å¡«å……å·¥ä½œæ± 
	for i := 0; i < parallelWorkers; i++ {
		workerPool <- struct{}{}
	}

	// åˆ›å»ºä¸Šä¸‹æ–‡
	ctx, cancel := context.WithCancel(context.Background())

	// åˆ›å»ºè‡ªé€‚åº”CUDAåŠ é€Ÿå™¨
	cudaAccelerator, err := NewAdaptiveCUDAAccelerator(0, memoryPoolGB)
	if err != nil {
		fmt.Printf("âš ï¸ è‡ªé€‚åº”CUDAåŠ é€Ÿå™¨åˆ›å»ºå¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†æ¨¡å¼: %v\n", err)
		cudaAccelerator = nil
	}

	vo := &VideoOptimization{
		batchSize:       batchSize,
		preprocessBuf:   preprocessBuf,
		imagePool:       imagePool,
		enableGPU:       true,
		maxBatchSize:    maxBatchSize,
		preprocessPool:  preprocessPool,
		resultPool:      resultPool,
		parallelWorkers: parallelWorkers,
		memoryBuffer:    memoryBuffer,
		asyncQueue:      asyncQueue,
		processDone:     processDone,
		workerPool:      workerPool,
		cudaAccelerator: cudaAccelerator,
		enableCUDA:      cudaAccelerator != nil,
		cudaDeviceID:    0,
		circuitBreaker:  &CircuitBreaker{maxFailures: 10, timeout: 30 * time.Second, retryTimeout: 5 * time.Second},
		rateLimiter:     &RateLimiter{maxTokens: int64(maxBatchSize * 2), refillRate: int64(maxBatchSize)},
		resourceMonitor: &ResourceMonitor{maxMemory: memoryPoolGB * 1024 * 1024 * 1024, maxGoroutines: 1000, maxCPU: 90.0, checkInterval: time.Second},
		healthChecker:   &HealthChecker{checkInterval: 5 * time.Second, maxFailures: 5},
		metrics:         &PerformanceMetrics{minLatency: time.Hour},
		ctx:             ctx,
		cancel:          cancel,
		gcInterval:      gcInterval,
		lastGCTime:      time.Now(),
	}

	// å¯åŠ¨å¼‚æ­¥å·¥ä½œçº¿ç¨‹å’Œç›‘æ§
	vo.startAsyncWorkers()
	vo.startStabilityMonitors()

	return vo
}

// NewRTX4090VideoOptimization åˆ›å»ºRTX 4090ä¸“ç”¨è§†é¢‘ä¼˜åŒ–å®ä¾‹
func NewRTX4090VideoOptimization() *VideoOptimization {
	cpuCores := runtime.NumCPU()

	// RTX 4090ä¸“ç”¨é…ç½® - å……åˆ†åˆ©ç”¨24GBæ˜¾å­˜
	batchSize := cpuCores * 8       // å¤§æ‰¹å¤„ç†ï¼Œåˆ©ç”¨å¤§æ˜¾å­˜
	maxBatchSize := cpuCores * 16   // æå¤§æ‰¹å¤„ç†
	parallelWorkers := cpuCores * 6 // æ›´å¤šå¹¶è¡Œå·¥ä½œçº¿ç¨‹

	// é¢„åˆ†é…æ›´å¤§çš„å†…å­˜ç¼“å†²åŒºï¼Œå……åˆ†åˆ©ç”¨24GBæ˜¾å­˜
	preprocessBuf := make([][]float32, batchSize)
	memoryBuffer := make([][]float32, maxBatchSize)
	for i := range memoryBuffer {
		memoryBuffer[i] = make([]float32, 3*640*640) // 640x640ç¼“å†²åŒº
	}

	// åˆ›å»ºå¯¹è±¡æ± ï¼Œä½¿ç”¨æ›´å¤§çš„ç¼“å†²åŒº
	imagePool := &sync.Pool{
		New: func() interface{} {
			return make([]float32, 3*640*640)
		},
	}

	preprocessPool := &sync.Pool{
		New: func() interface{} {
			return make([]float32, 3*640*640)
		},
	}

	resultPool := &sync.Pool{
		New: func() interface{} {
			return make([]Detection, 0, 200) // æ›´å¤§çš„æ£€æµ‹ç»“æœé¢„åˆ†é…
		},
	}

	// åˆ›å»ºæ›´å¤§çš„å¼‚æ­¥å¤„ç†é˜Ÿåˆ—
	asyncQueue := make(chan *ProcessTask, maxBatchSize*4)
	processDone := make(chan *ProcessResult, maxBatchSize*4)
	workerPool := make(chan struct{}, parallelWorkers)

	// å¡«å……å·¥ä½œæ± 
	for i := 0; i < parallelWorkers; i++ {
		workerPool <- struct{}{}
	}

	// åˆ›å»ºä¸Šä¸‹æ–‡ç”¨äºä¼˜é›…å…³é—­
	ctx, cancel := context.WithCancel(context.Background())

	// åˆ›å»ºRTX 4090ä¸“ç”¨CUDAåŠ é€Ÿå™¨
	cudaAccelerator, err := NewRTX4090CUDAAccelerator(0)
	if err != nil {
		fmt.Printf("âš ï¸ RTX 4090 CUDAåŠ é€Ÿå™¨åˆ›å»ºå¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†æ¨¡å¼: %v\n", err)
		cudaAccelerator = nil
	}

	vo := &VideoOptimization{
		batchSize:       batchSize,
		preprocessBuf:   preprocessBuf,
		imagePool:       imagePool,
		enableGPU:       true,
		maxBatchSize:    maxBatchSize,
		preprocessPool:  preprocessPool,
		resultPool:      resultPool,
		parallelWorkers: parallelWorkers,
		memoryBuffer:    memoryBuffer,
		asyncQueue:      asyncQueue,
		processDone:     processDone,
		workerPool:      workerPool,
		cudaAccelerator: cudaAccelerator,
		enableCUDA:      cudaAccelerator != nil,
		cudaDeviceID:    0,
		circuitBreaker:  &CircuitBreaker{maxFailures: 10, timeout: 30 * time.Second, retryTimeout: 5 * time.Second},
		rateLimiter:     &RateLimiter{maxTokens: int64(maxBatchSize * 2), refillRate: int64(maxBatchSize)},
		resourceMonitor: &ResourceMonitor{maxMemory: 20 * 1024 * 1024 * 1024, maxGoroutines: 1000, maxCPU: 90.0, checkInterval: time.Second},
		healthChecker:   &HealthChecker{checkInterval: 5 * time.Second, maxFailures: 5},
		metrics:         &PerformanceMetrics{minLatency: time.Hour},
		ctx:             ctx,
		cancel:          cancel,
		gcInterval:      30, // RTX 4090æ˜¾å­˜å¤§ï¼Œå¯ä»¥å‡å°‘GCé¢‘ç‡
		lastGCTime:      time.Now(),
	}

	// å¯åŠ¨å¼‚æ­¥å·¥ä½œçº¿ç¨‹å’Œç›‘æ§
	vo.startAsyncWorkers()
	vo.startStabilityMonitors()

	return vo
}

// detectVRAMSize æ£€æµ‹æ˜¾å­˜å¤§å°ï¼ˆGBï¼‰
// ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥é€šè¿‡CUDA APIè·å–å‡†ç¡®ä¿¡æ¯
func detectVRAMSize() int {
	// è¿™é‡Œåº”è¯¥è°ƒç”¨CUDA APIè·å–å®é™…æ˜¾å­˜å¤§å°
	// ç›®å‰è¿”å›ä¸€ä¸ªä¼°ç®—å€¼ï¼Œå¯ä»¥æ ¹æ®GPUå‹å·åˆ¤æ–­
	// å®é™…å®ç°ä¸­åº”è¯¥ä½¿ç”¨ cudaMemGetInfo ç­‰API
	return 24 // é»˜è®¤å‡è®¾ä¸ºé«˜ç«¯GPU
}

// NewAdaptiveCUDAAccelerator åˆ›å»ºè‡ªé€‚åº”CUDAåŠ é€Ÿå™¨
// æ ¹æ®æ˜¾å­˜å¤§å°è‡ªåŠ¨è°ƒæ•´å†…å­˜æ± å’Œæ‰¹å¤„ç†é…ç½®
func NewAdaptiveCUDAAccelerator(deviceID int, memoryPoolGB int64) (*CUDAAccelerator, error) {
	// æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
	if !isCUDAAvailable() {
		return nil, fmt.Errorf("CUDAä¸å¯ç”¨")
	}

	cpuCores := runtime.NumCPU()

	// æ ¹æ®æ˜¾å­˜å¤§å°è°ƒæ•´æµæ•°é‡å’Œæ‰¹å¤„ç†å¤§å°
	var streamCount, batchSize int
	switch {
	case memoryPoolGB >= 20: // é«˜ç«¯GPU (RTX 4090, 3090)
		streamCount = cpuCores * 4
		batchSize = cpuCores * 64
	case memoryPoolGB >= 12: // ä¸­é«˜ç«¯GPU (RTX 4080, 3080 Ti)
		streamCount = cpuCores * 3
		batchSize = cpuCores * 48
	case memoryPoolGB >= 8: // ä¸­ç«¯GPU (RTX 3070 Ti, 3080)
		streamCount = cpuCores * 2
		batchSize = cpuCores * 32
	default: // å…¶ä»–GPU
		streamCount = cpuCores * 2
		batchSize = cpuCores * 16
	}

	// åˆ›å»ºå†…å­˜æ± 
	memoryPool, err := newCUDAMemoryPool(deviceID, memoryPoolGB*1024*1024*1024)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºè‡ªé€‚åº”CUDAå†…å­˜æ± å¤±è´¥: %v", err)
	}

	// åˆ›å»ºæµç®¡ç†å™¨
	streamManager, err := newCUDAStreamManager(streamCount)
	if err != nil {
		memoryPool.Destroy()
		return nil, fmt.Errorf("åˆ›å»ºè‡ªé€‚åº”CUDAæµç®¡ç†å™¨å¤±è´¥: %v", err)
	}

	// åˆ›å»ºé¢„å¤„ç†å™¨
	preprocessor, err := newCUDAPreprocessor(deviceID)
	if err != nil {
		streamManager.Destroy()
		memoryPool.Destroy()
		return nil, fmt.Errorf("åˆ›å»ºè‡ªé€‚åº”CUDAé¢„å¤„ç†å™¨å¤±è´¥: %v", err)
	}

	// åˆ›å»ºæ‰¹å¤„ç†å™¨
	batchProcessor, err := newCUDABatchProcessor(batchSize)
	if err != nil {
		preprocessor.Destroy()
		streamManager.Destroy()
		memoryPool.Destroy()
		return nil, fmt.Errorf("åˆ›å»ºè‡ªé€‚åº”CUDAæ‰¹å¤„ç†å™¨å¤±è´¥: %v", err)
	}

	// åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨
	performanceMonitor := newCUDAPerformanceMonitor()

	fmt.Printf("ğŸš€ è‡ªé€‚åº”CUDAåŠ é€Ÿå™¨å·²å¯ç”¨ï¼š%dGBå†…å­˜æ±  + %dæµ + %dæ‰¹å¤„ç†\n",
		memoryPoolGB, streamCount, batchSize)

	return &CUDAAccelerator{
		enabled:            true,
		deviceID:           deviceID,
		streamCount:        streamCount,
		memoryPool:         memoryPool,
		streamManager:      streamManager,
		preprocessor:       preprocessor,
		batchProcessor:     batchProcessor,
		performanceMonitor: performanceMonitor,
	}, nil
}

// NewRTX4090CUDAAccelerator åˆ›å»ºRTX 4090ä¸“ç”¨CUDAåŠ é€Ÿå™¨
func NewRTX4090CUDAAccelerator(deviceID int) (*CUDAAccelerator, error) {
	// æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
	if !isCUDAAvailable() {
		return nil, fmt.Errorf("CUDAä¸å¯ç”¨")
	}

	cpuCores := runtime.NumCPU()
	streamCount := cpuCores * 4 // RTX 4090å¯ä»¥æ”¯æŒæ›´å¤šæµ

	// åˆ›å»ºæ›´å¤§çš„å†…å­˜æ±  - å……åˆ†åˆ©ç”¨RTX 4090çš„24GBæ˜¾å­˜
	memoryPool, err := newCUDAMemoryPool(deviceID, 20*1024*1024*1024) // 20GBå†…å­˜æ± 
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºRTX 4090 CUDAå†…å­˜æ± å¤±è´¥: %v", err)
	}

	// åˆ›å»ºæµç®¡ç†å™¨
	streamManager, err := newCUDAStreamManager(streamCount)
	if err != nil {
		memoryPool.Destroy()
		return nil, fmt.Errorf("åˆ›å»ºRTX 4090 CUDAæµç®¡ç†å™¨å¤±è´¥: %v", err)
	}

	// åˆ›å»ºé¢„å¤„ç†å™¨
	preprocessor, err := newCUDAPreprocessor(deviceID)
	if err != nil {
		streamManager.Destroy()
		memoryPool.Destroy()
		return nil, fmt.Errorf("åˆ›å»ºRTX 4090 CUDAé¢„å¤„ç†å™¨å¤±è´¥: %v", err)
	}

	// åˆ›å»ºæ‰¹å¤„ç†å™¨ - RTX 4090å¯ä»¥å¤„ç†æ›´å¤§çš„æ‰¹æ¬¡
	batchProcessor, err := newCUDABatchProcessor(cpuCores * 64) // è¶…å¤§æ‰¹å¤„ç†
	if err != nil {
		preprocessor.Destroy()
		streamManager.Destroy()
		memoryPool.Destroy()
		return nil, fmt.Errorf("åˆ›å»ºRTX 4090 CUDAæ‰¹å¤„ç†å™¨å¤±è´¥: %v", err)
	}

	// åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨
	performanceMonitor := newCUDAPerformanceMonitor()

	fmt.Println("ğŸš€ RTX 4090 CUDAåŠ é€Ÿå™¨å·²å¯ç”¨ï¼š20GBå†…å­˜æ±  + è¶…å¤§æ‰¹å¤„ç†")

	return &CUDAAccelerator{
		enabled:            true,
		deviceID:           deviceID,
		streamCount:        streamCount,
		memoryPool:         memoryPool,
		streamManager:      streamManager,
		preprocessor:       preprocessor,
		batchProcessor:     batchProcessor,
		performanceMonitor: performanceMonitor,
	}, nil
}

// HighEndGPUPerformanceTips é«˜ç«¯GPUæ€§èƒ½ä¼˜åŒ–å»ºè®®
func HighEndGPUPerformanceTips() {
	fmt.Println("\nğŸš€ é«˜ç«¯GPUæ€§èƒ½ä¼˜åŒ–å»ºè®®:")
	fmt.Println("1. ä½¿ç”¨ HighEndGPUOptimizedConfig() æˆ– NewAdaptiveGPUVideoOptimization() è‡ªåŠ¨é…ç½®")
	fmt.Println("2. æ ¹æ®æ˜¾å­˜å¤§å°é€‰æ‹©åˆé€‚çš„é…ç½®:")
	fmt.Println("   - RTX 4090/3090 (24GB): æ‰¹å¤„ç†128+, å†…å­˜æ± 20GB")
	fmt.Println("   - RTX 4080/3080Ti (12-16GB): æ‰¹å¤„ç†96+, å†…å­˜æ± 12GB")
	fmt.Println("   - RTX 3070Ti/3080 (8-10GB): æ‰¹å¤„ç†64+, å†…å­˜æ± 6GB")
	fmt.Println("3. å¹¶è¡Œå·¥ä½œçº¿ç¨‹: æ ¹æ®æ˜¾å­˜è‡ªåŠ¨è°ƒæ•´ (CPUæ ¸å¿ƒæ•° * 2-6)")
	fmt.Println("4. CUDAæµæ•°é‡: æ ¹æ®æ˜¾å­˜è‡ªåŠ¨è°ƒæ•´ (CPUæ ¸å¿ƒæ•° * 2-4)")
	fmt.Println("5. GCé—´éš”: æ˜¾å­˜è¶Šå¤§é—´éš”è¶Šé•¿ (15-30å¸§)")
	fmt.Println("6. ç¡®ä¿CUDA 11.8+ å’Œæœ€æ–°é©±åŠ¨")
	fmt.Println("7. å…³é—­ä¸å¿…è¦çš„åå°ç¨‹åºé‡Šæ”¾æ˜¾å­˜")
	fmt.Println("8. ä½¿ç”¨ TensorRT è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹")
	fmt.Println("9. ç›‘æ§GPUåˆ©ç”¨ç‡ï¼Œç¡®ä¿è¾¾åˆ°90%+")
	fmt.Println("10. è€ƒè™‘ä½¿ç”¨æ··åˆç²¾åº¦(FP16)æå‡æ€§èƒ½\n")
}

// RTX4090PerformanceTips RTX 4090æ€§èƒ½ä¼˜åŒ–å»ºè®®ï¼ˆå‘åå…¼å®¹ï¼‰
func RTX4090PerformanceTips() {
	HighEndGPUPerformanceTips()
}

// GetGPUBenchmarkConfig è·å–GPUåŸºå‡†æµ‹è¯•é…ç½®
// æ ¹æ®æ˜¾å­˜å¤§å°è¿”å›ç›¸åº”çš„æ€§èƒ½é¢„æœŸ
func GetGPUBenchmarkConfig(vramGB int) map[string]interface{} {
	cpuCores := runtime.NumCPU()

	switch {
	case vramGB >= 20: // RTX 4090, RTX 3090
		return map[string]interface{}{
			"gpu_tier":           "æ——èˆ°çº§ (RTX 4090/3090)",
			"vram_size":          fmt.Sprintf("%dGB", vramGB),
			"memory_pool_size":   "20GB",
			"batch_size":         cpuCores * 8,
			"max_batch_size":     cpuCores * 16,
			"parallel_workers":   cpuCores * 6,
			"cuda_streams":       cpuCores * 4,
			"gc_interval":        30,
			"expected_fps":       "300-500 (1000å¸§è§†é¢‘)",
			"target_time":        "10-20ç§’ (1000å¸§è§†é¢‘)",
			"optimization_level": "æè‡´",
		}
	case vramGB >= 12: // RTX 4080, RTX 3080 Ti
		return map[string]interface{}{
			"gpu_tier":           "é«˜ç«¯çº§ (RTX 4080/3080Ti)",
			"vram_size":          fmt.Sprintf("%dGB", vramGB),
			"memory_pool_size":   "12GB",
			"batch_size":         cpuCores * 6,
			"max_batch_size":     cpuCores * 12,
			"parallel_workers":   cpuCores * 4,
			"cuda_streams":       cpuCores * 3,
			"gc_interval":        25,
			"expected_fps":       "200-350 (1000å¸§è§†é¢‘)",
			"target_time":        "15-30ç§’ (1000å¸§è§†é¢‘)",
			"optimization_level": "é«˜çº§",
		}
	case vramGB >= 8: // RTX 3070 Ti, RTX 3080
		return map[string]interface{}{
			"gpu_tier":           "ä¸­é«˜ç«¯çº§ (RTX 3070Ti/3080)",
			"vram_size":          fmt.Sprintf("%dGB", vramGB),
			"memory_pool_size":   "6GB",
			"batch_size":         cpuCores * 4,
			"max_batch_size":     cpuCores * 8,
			"parallel_workers":   cpuCores * 3,
			"cuda_streams":       cpuCores * 2,
			"gc_interval":        20,
			"expected_fps":       "150-250 (1000å¸§è§†é¢‘)",
			"target_time":        "20-40ç§’ (1000å¸§è§†é¢‘)",
			"optimization_level": "ä¸­çº§",
		}
	default: // å…¶ä»–GPU
		return map[string]interface{}{
			"gpu_tier":           "æ ‡å‡†çº§",
			"vram_size":          fmt.Sprintf("%dGB", vramGB),
			"memory_pool_size":   "4GB",
			"batch_size":         cpuCores * 2,
			"max_batch_size":     cpuCores * 4,
			"parallel_workers":   cpuCores * 2,
			"cuda_streams":       cpuCores * 2,
			"gc_interval":        15,
			"expected_fps":       "100-180 (1000å¸§è§†é¢‘)",
			"target_time":        "30-60ç§’ (1000å¸§è§†é¢‘)",
			"optimization_level": "åŸºç¡€",
		}
	}
}

// RTX4090BenchmarkConfig RTX 4090åŸºå‡†æµ‹è¯•é…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
func RTX4090BenchmarkConfig() map[string]interface{} {
	return GetGPUBenchmarkConfig(24) // RTX 4090 æœ‰24GBæ˜¾å­˜
}

// GetOptimalGPUSettings è·å–å½“å‰GPUçš„æœ€ä¼˜è®¾ç½®å»ºè®®
func GetOptimalGPUSettings() map[string]interface{} {
	vramGB := detectVRAMSize()
	config := GetGPUBenchmarkConfig(vramGB)

	fmt.Printf("ğŸ” æ£€æµ‹åˆ°GPUé…ç½®: %s\n", config["gpu_tier"])
	fmt.Printf("ğŸ“Š é¢„æœŸæ€§èƒ½: %s\n", config["expected_fps"])
	fmt.Printf("â±ï¸  ç›®æ ‡å¤„ç†æ—¶é—´: %s\n", config["target_time"])

	return config
}
